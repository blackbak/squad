{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blackbak\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gensim\n",
    "from text_prep import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_sequence, pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/blackbak/Documents/github/data/squad_data/train-v2.0.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m =0\n",
    "questions = []\n",
    "for i in range(len(data[\"data\"])):\n",
    "    for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "        for k in range(len(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])):\n",
    "            questions.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"][k][\"question\"]+\" </s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When did Beyonce start becoming popular? </s>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = questions\n",
    "sentence_list = [unicodeToAscii(sentence) for sentence in sentence_list]\n",
    "tokenized_list = [sentence.lower().split() for sentence in sentence_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4651829, 7020590)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new = Word2Vec(size=100, min_count = 1)\n",
    "model_new.build_vocab(tokenized_list)\n",
    "total_examples = model_new.corpus_count\n",
    "model_new.train(tokenized_list, total_examples=total_examples, epochs=model_new.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_local_w2v(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_idx = [sentence2idx(model, sentence) for sentence in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu:0\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 8, 270, 143, 1478, 229, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = gensim2embedding(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embed_dimension, hidden_size, num_layers):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embed_dimension = embed_dimension\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size=embed_dimension, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(2*hidden_size, 300) # since we want bidirectional\n",
    "        self.fc2 = nn.Linear(300, 1)\n",
    "        self.activation_fc1 = nn.SELU()\n",
    "        self.activation_fc2 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, padded_input, input_lengths, batch_size):\n",
    "        #total_length = padded_input.size(1) #padded_input must be ordered by size\n",
    "        packed_input = pack_padded_sequence(padded_input, input_lengths, batch_first=True)\n",
    "        packed_output, last_hidden = self.rnn(packed_input)\n",
    "        last_hidden = last_hidden.permute(1, 0, 2)\n",
    "        last_hidden = last_hidden[:, -2:, :].reshape([batch_size,-1])\n",
    "        #gru_output, sequence_length = pad_packed_sequence(packed_output, batch_first=True, total_length = total_length)\n",
    "        fc1 = self.activation_fc1(self.fc1(last_hidden))\n",
    "        discriminator_output = self.activation_fc2(self.fc2(fc1))\n",
    "        return discriminator_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embed_dimension, hidden_size, num_layers):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embed_dimension = embed_dimension\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size=embed_dimension, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(2*hidden_size, 300) # since we want bidirectional\n",
    "        self.fc2 = nn.Linear(300, 1)\n",
    "        self.activation_fc1 = nn.SELU()\n",
    "        self.activation_fc2 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, padded_input, input_lengths, batch_size):\n",
    "        total_length = padded_input.shape[1] #padded_input must be ordered by size\n",
    "        packed_input = pack_padded_sequence(padded_input, input_lengths, batch_first=True)\n",
    "        packed_output, last_hidden = self.rnn(packed_input)\n",
    "        gru_output, sequence_length = pad_packed_sequence(packed_output, total_length = total_length)\n",
    "        last_output = gru_output[total_length-1, :, :]\n",
    "        fc1 = self.activation_fc1(self.fc1(last_output))\n",
    "        discriminator_output = self.activation_fc2(self.fc2(fc1))\n",
    "        return discriminator_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, embed_dimension, num_layers, embedding, eos, sos):\n",
    "        super(Generator, self).__init__()\n",
    "        self.eos_token = eos\n",
    "        self.sos_token = sos\n",
    "        self.noise_dim = noise_dim\n",
    "        self.embed_dimension = embed_dimension\n",
    "        self.hidden_size = embed_dimension\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = embedding\n",
    "        self.noise2hidden = nn.Linear(noise_dim, num_layers*embed_dimension)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.rnn = nn.GRU(input_size=embed_dimension, hidden_size=embed_dimension, num_layers=num_layers, batch_first=True)\n",
    "    \n",
    "    def most_similar(self, emb_input):\n",
    "        cos = nn.CosineSimilarity(dim=1)\n",
    "        similarity = cos(self.embedding.weight, emb_input.squeeze())\n",
    "        #similarity = torch.mv(self.embedding.weight, emb_input.squeeze())\n",
    "        value, idx = torch.max(similarity, 0)\n",
    "        return idx\n",
    "    \n",
    "    def forward(self, device):\n",
    "        noise = torch.randn(self.noise_dim).view(1,1,-1).to(device)\n",
    "        h0 = self.tanh(self.noise2hidden(noise)).view([self.num_layers, 1, self.embed_dimension])\n",
    "        #List that holds all the output (words/embeddings)\n",
    "        output = []\n",
    "        word_output = []\n",
    "        #First output with input the sos token\n",
    "        o, h = self.rnn(self.sos_token.view(1,1,-1), h0)\n",
    "        idx = self.most_similar(o)\n",
    "        output.append(o)\n",
    "        word_output.append(idx)\n",
    "        #We want to iterate for output some and then produce an eos token. Maximum length of\n",
    "        #the output we set it to 20 words or if we produce an eos token before that \n",
    "        for i in range(20):\n",
    "            o, h = self.rnn(o, h)\n",
    "            #might need to squeeze the output\n",
    "            #dot product to calculate similarity\n",
    "            #similarity = torch.mv(self.embedding, o)\n",
    "            idx = self.most_similar(o)\n",
    "            output.append(o)\n",
    "            word_output.append(idx)\n",
    "            if idx==self.eos_token:\n",
    "                break\n",
    "        #here we have the option to append the eos token or not\n",
    "        return torch.cat(output), word_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, embed_dimension, num_layers):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.embed_dimension = embed_dimension\n",
    "        self.hidden_size = embed_dimension\n",
    "        self.num_layers = num_layers\n",
    "        self.noise2hidden = nn.Linear(noise_dim, num_layers*embed_dimension)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.rnn = nn.GRU(input_size=embed_dimension, hidden_size=embed_dimension, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, o, noise=None, h=None):\n",
    "        if h is None:\n",
    "            h0 = self.tanh(self.noise2hidden(noise)).view([self.num_layers, 1, self.embed_dimension])\n",
    "            o, h = self.rnn(o, h0) #here is self.sos_token.view(1,1,-1)\n",
    "        else:\n",
    "            o, h = self.rnn(o, h)\n",
    "        return o, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, embed_dimension, num_layers):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.embed_dimension = embed_dimension\n",
    "        self.hidden_size = embed_dimension\n",
    "        self.num_layers = num_layers\n",
    "        self.noise2hidden = nn.Linear(noise_dim, num_layers*embed_dimension)\n",
    "        self.tanh = nn.Tanh()\n",
    "        #batch first does not work on autoregressive rnn\n",
    "        self.rnn = nn.GRU(input_size=embed_dimension, hidden_size=embed_dimension, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, o, h):\n",
    "        o, h = self.rnn(o, h)\n",
    "        return o, h\n",
    "    \n",
    "    def init_hidden(self, noise):\n",
    "        return self.tanh(self.noise2hidden(noise)).view([self.num_layers, 1, self.embed_dimension])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(embedding, emb_input):\n",
    "    #cos = nn.CosineSimilarity(dim=1)\n",
    "    #similarity = cos(embedding.weight, emb_input.view(1, -1))\n",
    "    similarity = torch.mv(embedding.weight, emb_input.squeeze())\n",
    "    value, idx = torch.max(similarity, 0)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, input_sequence, embedding, generator_optimizer, discriminator_optimizer, criterion, eos):\n",
    "    #input sequence of shape [1,seq_len,300]\n",
    "    ###Discriminator training\n",
    "    #train with real data\n",
    "    discriminator_optimizer.zero_grad()\n",
    "    real_output = discriminator.forward(padded_input=input_sequence.to(device), input_lengths=torch.tensor([input_sequence.shape[1]], device=device), batch_size=1)\n",
    "    real_label = torch.ones(1, device=device)\n",
    "    real_error = criterion(real_output, real_label)\n",
    "    real_error.backward()\n",
    "    #train with fake\n",
    "    #generate sequence\n",
    "    generated_sequence = []\n",
    "    generated_idx = []\n",
    "    noise = torch.randn(generator.noise_dim).to(device)\n",
    "    o_gen = torch.zeros(embedding.weight.shape[1], device=device).view(1,1,-1)\n",
    "    h_gen = generator.init_hidden(noise)\n",
    "    for i in range(20):\n",
    "        o_gen, h_gen = generator.forward(o_gen, h_gen)\n",
    "        generated_sequence.append(o_gen)\n",
    "        idx = most_similar(embedding, o_gen)\n",
    "        generated_idx.append(idx)\n",
    "        if idx==eos:\n",
    "            break\n",
    "    generated_sequence = torch.cat(generated_sequence).view(1, -1, embedding.weight.shape[1])\n",
    "    fake_output = discriminator.forward(padded_input=generated_sequence.detach(), input_lengths=torch.tensor([generated_sequence.shape[1]], device=device), batch_size=1)\n",
    "    fake_label = torch.zeros(1, device=device)\n",
    "    fake_error = criterion(fake_output, fake_label)\n",
    "    fake_error.backward()\n",
    "    discriminator_error = real_error + fake_error\n",
    "    discriminator_optimizer.step()\n",
    "    ###Generator training\n",
    "    generator_optimizer.zero_grad()\n",
    "    fake_output_gen = discriminator.forward(padded_input=generated_sequence, input_lengths=torch.tensor([generated_sequence.shape[1]], device=device), batch_size=1)\n",
    "    fake_labels_gen = torch.ones(1, device=device)\n",
    "    generator_error = criterion(fake_output_gen, fake_labels_gen)\n",
    "    generator_error.backward()\n",
    "    generator_optimizer.step()\n",
    "    loss = discriminator_error + generator_error\n",
    "    return loss.item(), generator_error.item(), discriminator_error.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(generator, discriminator, dataset, embedding, eos, epochs, lr_g=0.001, lr_d=0.00001):\n",
    "    #we have to append <\\s> to each question at the end\n",
    "    total_loss = []\n",
    "    generator_optimizer = optim.Adam(generator.parameters(), lr=lr_g)\n",
    "    discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=lr_d)\n",
    "    criterion = nn.BCELoss()\n",
    "    for e in range(epochs):\n",
    "        loss = 0\n",
    "        gen_loss = 0\n",
    "        dis_loss = 0\n",
    "        for i, data in enumerate(dataset):\n",
    "            embeds = embedding(torch.tensor(data+[eos], device=device)).view(1, -1, embedding.weight.shape[1])\n",
    "            current_loss, curr_gen_loss, curr_dis_loss = train(generator=generator, discriminator=discriminator, input_sequence=embeds, \n",
    "                                 embedding=embedding, generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer, criterion=criterion, eos=eos)\n",
    "            loss += current_loss\n",
    "            gen_loss += curr_gen_loss\n",
    "            dis_loss += curr_dis_loss\n",
    "            if i%500==0:\n",
    "                total_loss.append(loss)\n",
    "                print(\"Loss at iteration {}: {}\".format(i, loss))\n",
    "                print(\"Gen loss at iteration {}: {}\".format(i, gen_loss))\n",
    "                print(\"Dis loss at iteration {}: {}\".format(i, dis_loss))\n",
    "                loss = 0\n",
    "                gen_loss = 0\n",
    "                dis_loss = 0\n",
    "\n",
    "    plot_loss(total_loss)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_list):\n",
    "    plt.figure()\n",
    "    plt.plot(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (noise2hidden): Linear(in_features=20, out_features=1000, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (rnn): GRU(100, 100, num_layers=10)\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator(noise_dim=20, embed_dimension=100, num_layers=10)\n",
    "generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (rnn): GRU(100, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=64, out_features=300, bias=True)\n",
       "  (fc2): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (activation_fc1): SELU()\n",
       "  (activation_fc2): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = Discriminator(embed_dimension=100, hidden_size=32, num_layers=2)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos = word2idx(model, \"</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blackbak\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1594: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0: 2.076388359069824\n",
      "Gen loss at iteration 0: 0.6562976837158203\n",
      "Dis loss at iteration 0: 1.4200905561447144\n",
      "Loss at iteration 500: 1009.8449165821075\n",
      "Gen loss at iteration 500: 262.75751584768295\n",
      "Dis loss at iteration 500: 747.0874003171921\n",
      "Loss at iteration 1000: 975.7422472238541\n",
      "Gen loss at iteration 1000: 301.814466714859\n",
      "Dis loss at iteration 1000: 673.9277812242508\n",
      "Loss at iteration 1500: 931.5117551088333\n",
      "Gen loss at iteration 1500: 306.5322377681732\n",
      "Dis loss at iteration 1500: 624.9795179367065\n",
      "Loss at iteration 2000: 895.715315580368\n",
      "Gen loss at iteration 2000: 292.915066331625\n",
      "Dis loss at iteration 2000: 602.8002481460571\n",
      "Loss at iteration 2500: 914.0657157897949\n",
      "Gen loss at iteration 2500: 336.04802763462067\n",
      "Dis loss at iteration 2500: 578.0176884531975\n",
      "Loss at iteration 3000: 842.7843374013901\n",
      "Gen loss at iteration 3000: 355.9365492463112\n",
      "Dis loss at iteration 3000: 486.8477868437767\n",
      "Loss at iteration 3500: 846.7635543346405\n",
      "Gen loss at iteration 3500: 390.3604062795639\n",
      "Dis loss at iteration 3500: 456.4031490087509\n",
      "Loss at iteration 4000: 840.0516042709351\n",
      "Gen loss at iteration 4000: 443.26722049713135\n",
      "Dis loss at iteration 4000: 396.7843841910362\n",
      "Loss at iteration 4500: 875.034642457962\n",
      "Gen loss at iteration 4500: 560.7912065386772\n",
      "Dis loss at iteration 4500: 314.24343517422676\n",
      "Loss at iteration 5000: 848.7633259296417\n",
      "Gen loss at iteration 5000: 596.7287386655807\n",
      "Dis loss at iteration 5000: 252.0345872938633\n",
      "Loss at iteration 5500: 826.4816607236862\n",
      "Gen loss at iteration 5500: 564.2532922625542\n",
      "Dis loss at iteration 5500: 262.2283685207367\n",
      "Loss at iteration 6000: 899.9634450674057\n",
      "Gen loss at iteration 6000: 700.0966178178787\n",
      "Dis loss at iteration 6000: 199.86682604253292\n",
      "Loss at iteration 6500: 1087.0123369693756\n",
      "Gen loss at iteration 6500: 961.5268512368202\n",
      "Dis loss at iteration 6500: 125.48548431694508\n",
      "Loss at iteration 7000: 1443.804094195366\n",
      "Gen loss at iteration 7000: 1389.353544473648\n",
      "Dis loss at iteration 7000: 54.45055065304041\n",
      "Loss at iteration 7500: 1755.7584931850433\n",
      "Gen loss at iteration 7500: 1725.4347525835037\n",
      "Dis loss at iteration 7500: 30.323743348941207\n",
      "Loss at iteration 8000: 1923.73912358284\n",
      "Gen loss at iteration 8000: 1900.4618109464645\n",
      "Dis loss at iteration 8000: 23.277313369326293\n",
      "Loss at iteration 8500: 2270.0879468917847\n",
      "Gen loss at iteration 8500: 2257.483315229416\n",
      "Dis loss at iteration 8500: 12.604629299603403\n",
      "Loss at iteration 9000: 2329.8073736429214\n",
      "Gen loss at iteration 9000: 2316.928010880947\n",
      "Dis loss at iteration 9000: 12.879366654902697\n",
      "Loss at iteration 9500: 2469.551449894905\n",
      "Gen loss at iteration 9500: 2459.8057433366776\n",
      "Dis loss at iteration 9500: 9.74570866022259\n",
      "Loss at iteration 10000: 2765.9699400663376\n",
      "Gen loss at iteration 10000: 2760.5708144903183\n",
      "Dis loss at iteration 10000: 5.399131865007803\n",
      "Loss at iteration 10500: 2905.846399784088\n",
      "Gen loss at iteration 10500: 2901.311707854271\n",
      "Dis loss at iteration 10500: 4.534696574788541\n",
      "Loss at iteration 11000: 3069.874023914337\n",
      "Gen loss at iteration 11000: 3066.254538536072\n",
      "Dis loss at iteration 11000: 3.6194845555583015\n",
      "Loss at iteration 11500: 3200.0685155391693\n",
      "Gen loss at iteration 11500: 3197.1396676301956\n",
      "Dis loss at iteration 11500: 2.9288433813489974\n",
      "Loss at iteration 12000: 3343.974729537964\n",
      "Gen loss at iteration 12000: 3341.898936986923\n",
      "Dis loss at iteration 12000: 2.0757921358163003\n",
      "Loss at iteration 12500: 3511.3817076683044\n",
      "Gen loss at iteration 12500: 3509.687086582184\n",
      "Dis loss at iteration 12500: 1.6946304805169348\n",
      "Loss at iteration 13000: 3650.924500465393\n",
      "Gen loss at iteration 13000: 3649.684510707855\n",
      "Dis loss at iteration 13000: 1.2399802036525216\n",
      "Loss at iteration 13500: 3834.3307766914368\n",
      "Gen loss at iteration 13500: 3833.3142082691193\n",
      "Dis loss at iteration 13500: 1.0165655274613528\n",
      "Loss at iteration 14000: 4001.4262692928314\n",
      "Gen loss at iteration 14000: 4000.4897270202637\n",
      "Dis loss at iteration 14000: 0.9365382518881233\n",
      "Loss at iteration 14500: 4251.734848737717\n",
      "Gen loss at iteration 14500: 4251.118652105331\n",
      "Dis loss at iteration 14500: 0.6161881099396851\n",
      "Loss at iteration 15000: 4364.303362131119\n",
      "Gen loss at iteration 15000: 4363.785282373428\n",
      "Dis loss at iteration 15000: 0.5180772030798835\n",
      "Loss at iteration 15500: 4591.740396022797\n",
      "Gen loss at iteration 15500: 4591.436770915985\n",
      "Dis loss at iteration 15500: 0.3036153225439193\n",
      "Loss at iteration 16000: 4693.57794046402\n",
      "Gen loss at iteration 16000: 4693.273458003998\n",
      "Dis loss at iteration 16000: 0.3044686317407468\n",
      "Loss at iteration 16500: 4838.241576194763\n",
      "Gen loss at iteration 16500: 4837.974276065826\n",
      "Dis loss at iteration 16500: 0.267290721540121\n",
      "Loss at iteration 17000: 5148.911237716675\n",
      "Gen loss at iteration 17000: 5148.752258777618\n",
      "Dis loss at iteration 17000: 0.1589655049974681\n",
      "Loss at iteration 17500: 5270.668801784515\n",
      "Gen loss at iteration 17500: 5270.545405387878\n",
      "Dis loss at iteration 17500: 0.12338551369612105\n",
      "Loss at iteration 18000: 5393.869818210602\n",
      "Gen loss at iteration 18000: 5393.756238937378\n",
      "Dis loss at iteration 18000: 0.11356063217499468\n",
      "Loss at iteration 18500: 5552.515955924988\n",
      "Gen loss at iteration 18500: 5552.428791999817\n",
      "Dis loss at iteration 18500: 0.08715409100204852\n",
      "Loss at iteration 19000: 5734.961465358734\n",
      "Gen loss at iteration 19000: 5734.908591270447\n",
      "Dis loss at iteration 19000: 0.05285543282934668\n",
      "Loss at iteration 19500: 5819.401147842407\n",
      "Gen loss at iteration 19500: 5819.3552803993225\n",
      "Dis loss at iteration 19500: 0.04585803762120122\n",
      "Loss at iteration 20000: 5982.281280040741\n",
      "Gen loss at iteration 20000: 5982.236445903778\n",
      "Dis loss at iteration 20000: 0.044821791870617744\n",
      "Loss at iteration 20500: 6128.065721988678\n",
      "Gen loss at iteration 20500: 6128.014486789703\n",
      "Dis loss at iteration 20500: 0.051225734403942624\n",
      "Loss at iteration 21000: 6330.079713821411\n",
      "Gen loss at iteration 21000: 6330.058067321777\n",
      "Dis loss at iteration 21000: 0.021627625746077683\n",
      "Loss at iteration 21500: 6424.390331745148\n",
      "Gen loss at iteration 21500: 6424.36732006073\n",
      "Dis loss at iteration 21500: 0.022997532445288016\n",
      "Loss at iteration 22000: 6554.941339015961\n",
      "Gen loss at iteration 22000: 6554.925055027008\n",
      "Dis loss at iteration 22000: 0.01627099322513459\n",
      "Loss at iteration 22500: 6711.247815608978\n",
      "Gen loss at iteration 22500: 6711.21485710144\n",
      "Dis loss at iteration 22500: 0.03295793862116625\n",
      "Loss at iteration 23000: 6912.680785179138\n",
      "Gen loss at iteration 23000: 6912.669956207275\n",
      "Dis loss at iteration 23000: 0.010821213121801065\n",
      "Loss at iteration 23500: 6941.758337974548\n",
      "Gen loss at iteration 23500: 6941.745418548584\n",
      "Dis loss at iteration 23500: 0.012893405123804769\n",
      "Loss at iteration 24000: 7036.8774218559265\n",
      "Gen loss at iteration 24000: 7036.868420124054\n",
      "Dis loss at iteration 24000: 0.008992483227359571\n",
      "Loss at iteration 24500: 7145.902603149414\n",
      "Gen loss at iteration 24500: 7145.894561767578\n",
      "Dis loss at iteration 24500: 0.00802820518504177\n",
      "Loss at iteration 25000: 7235.855049133301\n",
      "Gen loss at iteration 25000: 7235.847598552704\n",
      "Dis loss at iteration 25000: 0.007425087351862203\n",
      "Loss at iteration 25500: 7404.139975070953\n",
      "Gen loss at iteration 25500: 7404.134104251862\n",
      "Dis loss at iteration 25500: 0.005873636816687622\n",
      "Loss at iteration 26000: 7602.2462458610535\n",
      "Gen loss at iteration 26000: 7602.242592334747\n",
      "Dis loss at iteration 26000: 0.003690208634566261\n",
      "Loss at iteration 26500: 7725.966957569122\n",
      "Gen loss at iteration 26500: 7725.96298789978\n",
      "Dis loss at iteration 26500: 0.004008888178091752\n",
      "Loss at iteration 27000: 7807.798134803772\n",
      "Gen loss at iteration 27000: 7807.795762062073\n",
      "Dis loss at iteration 27000: 0.0024259484571729217\n",
      "Loss at iteration 27500: 7881.379866600037\n",
      "Gen loss at iteration 27500: 7881.376461029053\n",
      "Dis loss at iteration 27500: 0.00346399793438934\n",
      "Loss at iteration 28000: 8059.896694660187\n",
      "Gen loss at iteration 28000: 8059.8947649002075\n",
      "Dis loss at iteration 28000: 0.002017845727394274\n",
      "Loss at iteration 28500: 8236.384147644043\n",
      "Gen loss at iteration 28500: 8236.382906913757\n",
      "Dis loss at iteration 28500: 0.001320385583895245\n",
      "Loss at iteration 29000: 8463.224431037903\n",
      "Gen loss at iteration 29000: 8463.223501205444\n",
      "Dis loss at iteration 29000: 0.0009989008004254174\n",
      "Loss at iteration 29500: 8455.170696258545\n",
      "Gen loss at iteration 29500: 8455.169521331787\n",
      "Dis loss at iteration 29500: 0.001256092343979276\n",
      "Loss at iteration 30000: 8611.155587673187\n",
      "Gen loss at iteration 30000: 8611.154602050781\n",
      "Dis loss at iteration 30000: 0.0010514672685673077\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-0b494a1cd94b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m train_iter(generator=generator, discriminator=discriminator, \n\u001b[1;32m----> 2\u001b[1;33m            dataset=questions_idx, embedding=embedding, eos=eos, epochs=1, lr_g=0.01, lr_d=0.00001)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-117-05356bed4f05>\u001b[0m in \u001b[0;36mtrain_iter\u001b[1;34m(generator, discriminator, dataset, embedding, eos, epochs, lr_g, lr_d)\u001b[0m\n\u001b[0;32m     13\u001b[0m             current_loss, curr_gen_loss, curr_dis_loss = train(generator=generator, discriminator=discriminator, input_sequence=embeds, \n\u001b[0;32m     14\u001b[0m                                  \u001b[0membedding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator_optimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator_optimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                                  discriminator_optimizer=discriminator_optimizer, criterion=criterion, eos=eos)\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcurrent_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mgen_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcurr_gen_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-6222516a4319>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(generator, discriminator, input_sequence, embedding, generator_optimizer, discriminator_optimizer, criterion, eos)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mh_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mo_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mgenerated_sequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-116-f2eef53d8460>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, o, h)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         )\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, weight, hx, batch_sizes)\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[0mbatch_first\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mvariable_length\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             dropout_ts)\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_iter(generator=generator, discriminator=discriminator, \n",
    "           dataset=questions_idx, embedding=embedding, eos=eos, epochs=1, lr_g=0.01, lr_d=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(generator, embedding, model):\n",
    "    generated_sequence = []\n",
    "    generated_idx = []\n",
    "    generated_words = []\n",
    "    noise = torch.randn(generator.noise_dim).to(device)\n",
    "    o_gen = torch.zeros(embedding.weight.shape[1], device=device).view(1,1,-1)\n",
    "    h_gen = generator.init_hidden(noise)\n",
    "    for i in range(20):\n",
    "        o_gen, h_gen = generator.forward(o_gen, h_gen)\n",
    "        generated_sequence.append(o_gen)\n",
    "        idx = most_similar(embedding, o_gen)\n",
    "        generated_idx.append(idx)\n",
    "        generated_words.append(idx2word(model, idx))\n",
    "        if idx==eos:\n",
    "            break\n",
    "    return generated_words, generated_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, idx = generate_question(generator, embedding, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city',\n",
       " 'year',\n",
       " 'been',\n",
       " 'is',\n",
       " 'is',\n",
       " 'is',\n",
       " 'is',\n",
       " 'is',\n",
       " 'is',\n",
       " 'is',\n",
       " 'is',\n",
       " 'is',\n",
       " 'are',\n",
       " 'is',\n",
       " 'is',\n",
       " 'is',\n",
       " 'is',\n",
       " 'is',\n",
       " 'is',\n",
       " 'is']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### some changes for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
